cfg_model:
    pre_model_name: Qwen
    pre_instruct_bool: False
    pre_qlora_bool: True
cfg_lora:
    lora_r: 8
    lora_lora_alpha: 16
    lora_lora_dropout: 0.05
    lora_target_modules : ["q_proj", "k_proj", "v_proj", "o_proj","gate_proj", "up_proj", "down_proj",]
    lora_bias: "none"
    lora_task_type: "CAUSAL_LM"
    lora_modules_to_save: ["embed_tokens"]
cfg_training_arguments:
    ta_overwrite_output_dir: True
    ta_per_device_train_batch_size: 2
    ta_per_device_eval_batch_size: 4
    ta_gradient_accumulation_steps: 1
    ta_eval_accumulation_steps: 1
    ta_learning_rate: 2e-5
    ta_weight_decay: 0.01
    ta_num_train_epochs: 6
    ta_evaluation_strategy : 'steps'
    ta_eval_steps : 200
    ta_logging_steps: 200
    ta_lr_scheduler_type: "constant",
    ta_warmup_steps: 0
    ta_save_strategy: "epoch"
    ta_save_steps: 1
    ta_fp16: True
    ta_report_to: "wandb"
    ta_optim : "adamw_8bit" # low memory optimizer
